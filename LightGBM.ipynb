{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict which clients are more likely to default on their loans\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = '/kaggle/input/home-credit-default-risk-model-stability'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    # calculate and display the memory usage of the dataframe\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\").format(start_mem)\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    #Standardize the dtype\n",
    "    def set_table_dtypes(df): \n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    #Change the feature for D to the difference in days from date_decision.\n",
    "    def handle_dates(df): \n",
    "        for col in df.columns:\n",
    "            if (col[-1] in (\"D\",)) and ('count' not in col):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    #Remove those with an average is_null exceeding 0.95 and those that do not fall within the range 1 < nunique < 200.\n",
    "    def filter_cols(df): \n",
    "        for col in df.columns:\n",
    "            # if col in [\"decision_month\", \"decision_weekday\"]:\n",
    "            #     df = df.drop(col)\n",
    "            #     continue\n",
    "            # if ('amtde' in col) or ('bureau_b2' in col): # for ohter option\n",
    "            #     continue\n",
    "\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            # if '_depth2_' in col:\n",
    "            #     continue\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\", ]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 50):#50 #len(df) * 0.20): # 95 # fe4 down at fq20\n",
    "                    df = df.drop(col)\n",
    "            \n",
    "            # eliminate year, month feature\n",
    "            # 644\n",
    "            if (col[-1] not in [\"P\", \"A\", \"L\", \"M\"]) and (('month_' in col) or ('year_' in col)):# or ('num_group' in col):\n",
    "            # if (('month_' in col) or ('year_' in col)):# or ('num_group' in col):\n",
    "                df = df.drop(col)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        expr_2 = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "\n",
    "        cols2 = [col for col in df.columns if col[-1] in (\"L\", \"A\")]\n",
    "        expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2] + [pl.std(col).alias(f\"std_{col}\") for col in cols2] + \\\n",
    "            [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] + [pl.median(col).alias(f\"median_{col}\") for col in cols2] # + \\\n",
    "   \n",
    "        return expr_1 + expr_2 + expr_3 # + [pl.col(col).diff().last().alias(f\"diff-last_{col}\") for col in cols3] # + expr_4\n",
    "    \n",
    "    @staticmethod\n",
    "    def applprev2_exprs(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" not in col]\n",
    "        expr_2 = [pl.first(col).alias(f\"first_{col}\") for col in cols]#  + [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        return []#expr_2\n",
    "\n",
    "    @staticmethod\n",
    "    def bureau_a1(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        expr_2 = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "\n",
    "        cols2 = [\n",
    "        'annualeffectiverate_199L', 'annualeffectiverate_63L',\n",
    "        'contractsum_5085717L', \n",
    "        'credlmt_230A', 'credlmt_935A',\n",
    "       'nominalrate_281L', 'nominalrate_498L',\n",
    "       'numberofcontrsvalue_258L', 'numberofcontrsvalue_358L',\n",
    "       'numberofinstls_229L', 'numberofinstls_320L',\n",
    "       'numberofoutstandinstls_520L', 'numberofoutstandinstls_59L',\n",
    "       'numberofoverdueinstlmax_1039L', 'numberofoverdueinstlmax_1151L',\n",
    "       'numberofoverdueinstls_725L', 'numberofoverdueinstls_834L',\n",
    "       ]\n",
    "\n",
    "        expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2] + [pl.std(col).alias(f\"std_{col}\") for col in cols2] + \\\n",
    "            [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] + [pl.median(col).alias(f\"median_{col}\") for col in cols2] + \\\n",
    "            [pl.first(col).alias(f\"first_{col}\") for col in cols2] # + [pl.last(col).alias(f\"last_{col}\") for col in cols2] # not applied\n",
    "        \n",
    "        return expr_1 + expr_2 + expr_3    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bureau_b1(df): \n",
    "        return []\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def bureau_b2(df):  \n",
    "        return []\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def deposit_exprs(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] # + \\\n",
    "\n",
    "        return expr_1 # + expr_2 #+ expr_ngmax\n",
    "\n",
    "    @staticmethod\n",
    "    def debitcard_exprs(df):\n",
    "        # cols = [col for col in df.columns if (col[-1] in [\"A\"])]\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] \n",
    "\n",
    "        return expr_1 \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def person_expr(df):\n",
    "        cols1 = ['empl_employedtotal_800L', 'empl_employedfrom_271D', 'empl_industry_691L', \n",
    "                 'familystate_447L', 'incometype_1044T', 'sex_738L', 'housetype_905L', 'housingtype_772L',\n",
    "                 'isreference_387L', 'birth_259D', ]\n",
    "        expr_1 = [pl.first(col).alias(f\"first_{col}\") for col in cols1]\n",
    "        \n",
    "        expr_2 = [pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"), \n",
    "                  pl.col(\"mainoccupationinc_384A\").filter(pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")]\n",
    "        \n",
    "        return expr_1 + expr_2\n",
    "    \n",
    "    @staticmethod\n",
    "    def person_2_expr(df):\n",
    "        # cols = [col for col in df.columns]\n",
    "        cols = ['empls_economicalst_849M', 'empls_employedfrom_796D', 'empls_employer_name_740M']\n",
    "\n",
    "        expr_1 = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_2 = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "\n",
    "        return expr_1 + expr_2 # + expr_3# + expr_ngc \n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        expr_1 = [pl.first(col).alias(f\"__other_{col}\") for col in df.columns if ('num_group' not in col) and (col != 'case_id')]\n",
    "        return expr_1\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def tax_a_exprs(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] + \\\n",
    "            [pl.last(col).alias(f\"last_{col}\") for col in cols] + \\\n",
    "            [pl.first(col).alias(f\"first_{col}\") for col in cols] + \\\n",
    "            [pl.mean(col).alias(f\"mean_{col}\") for col in cols] + \\\n",
    "            [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "\n",
    "        expr_4 = [pl.col(col).fill_null(strategy=\"zero\").apply(lambda x: x.max() - x.min()).alias(f\"max-min_gap_depth2_{col}\") for col in ['amount_4527230A']]\n",
    "\n",
    "        return expr_1 + expr_4\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bureau_a2(df): \n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "\n",
    "        expr_1 = [pl.max(col).alias(f\"max_depth2_{col}\") for col in cols]\n",
    "        expr_2 = [pl.min(col).alias(f\"min_depth2_{col}\") for col in cols]\n",
    "        expr_3 = [pl.mean(col).alias(f\"mean_depth2_{col}\") for col in cols] + \\\n",
    "            [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "\n",
    "        expr_4 = [pl.col(col).fill_null(strategy=\"zero\").apply(lambda x: x.max() - x.min()).alias(f\"max-min_gap_depth2_{col}\") for col in ['collater_valueofguarantee_1124L', 'pmts_dpd_1073P', 'pmts_overdue_1140A',]]\n",
    "\n",
    "        expr_ngc = [pl.count(\"num_group2\").alias(f\"count_depth2_a2_num_group2\")]\n",
    "\n",
    "        return expr_1 + expr_2 + expr_3 + expr_4 + expr_ngc \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df)\n",
    "\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_case(path, df):\n",
    "    path = str(path)\n",
    "    if '_applprev_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "\n",
    "    elif '_credit_bureau_a_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.bureau_a1(df))\n",
    "\n",
    "    elif '_credit_bureau_b_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.bureau_b1(df))\n",
    "\n",
    "    elif '_deposit_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.deposit_exprs(df))\n",
    "    elif '_debitcard_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.debitcard_exprs(df))\n",
    "        \n",
    "    elif '_tax_registry_a' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.tax_a_exprs(df))\n",
    "    elif '_tax_registry_b' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    elif '_tax_registry_c' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "    elif '_other_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.other_expr(df))\n",
    "    elif '_person_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.person_expr(df))\n",
    "    elif '_person_2' in path:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.person_2_expr(df))\n",
    "\n",
    "    elif '_credit_bureau_a_2' in path:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.bureau_a2(df))\n",
    "    elif '_credit_bureau_b_2' in path:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_file(path, depth=None): \n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = agg_by_case(path, df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    print(regex_path)\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = agg_by_case(path, df)\n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base.with_columns(\n",
    "            decision_month = pl.col(\"date_decision\").dt.month(),\n",
    "            decision_weekday = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    print(df_data.info())\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_person_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = feature_eng(**data_store)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "# load models\n",
    "cat_cols = joblib.load('/kaggle/input/hclgb-models/cat_cols.pickle')\n",
    "ls_models = glob(os.path.join('/kaggle/input/hclgb-models', \"*.pkl\"))\n",
    "models = [joblib.load(fn) for fn in ls_models]\n",
    "\n",
    "lgb_features = models[0].feature_name_\n",
    "\n",
    "df_test = df_test.select(['case_id'] + lgb_features)\n",
    "\n",
    "# print(\"train data shape:\\t\", df_train.shape)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "df_test, _ = to_pandas(df_test, cat_cols)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "df_test.drop(columns=[\"case_id\"]).to_csv('test_data.csv', index=False)\n",
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n",
    "\n",
    "# load models\n",
    "cat_cols = joblib.load('/kaggle/input/hclgb-models/cat_cols.pickle')\n",
    "ls_models = glob(os.path.join('/kaggle/input/hclgb-models', \"*.pkl\"))\n",
    "models = [joblib.load(fn) for fn in ls_models]\n",
    "\n",
    "print(len(models), models)\n",
    "\n",
    "lgb_features = models[0].feature_name_\n",
    "len(lgb_features), lgb_features\n",
    "\n",
    "def set_categoricals(df_data, cat_cols):\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data\n",
    "\n",
    "def lgb_prediction(feats, models):\n",
    "    predictions = np.zeros(len(feats))\n",
    "    for model in models:\n",
    "        p = model.predict_proba(feats)[:, 1]\n",
    "        predictions += p/len(models)\n",
    "    return predictions\n",
    "\n",
    "CHUNK_SIZE = 10 ** 6\n",
    "reader = pd.read_csv('/kaggle/working/test_data.csv', chunksize=CHUNK_SIZE)\n",
    "y_pred = []\n",
    "for df_chunk in reader:\n",
    "    # p = predictor.predict_proba(df_chunk).iloc[:, 1].values\n",
    "    df_chunk = set_categoricals(df_chunk, cat_cols)\n",
    "    p = lgb_prediction(df_chunk, models)\n",
    "    y_pred.append(p)\n",
    "    \n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "del reader\n",
    "gc.collect()\n",
    "\n",
    "df_subm = pd.read_csv(f\"{ROOT}/sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred#lgb_pred\n",
    "\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "print(df_subm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
