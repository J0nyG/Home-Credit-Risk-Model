{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "save_full_path = '/kaggle/input/hc-catboost-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/hc-catboost-models/cat_cols.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cat_cols \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_full_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/cat_cols.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ls_models \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_full_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcatboost_model_fold_*\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m [CatBoostClassifier()\u001b[38;5;241m.\u001b[39mload_model(fn, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcbm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m ls_models]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/hc-catboost-models/cat_cols.pickle'"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "cat_cols = joblib.load(f'{save_full_path}/cat_cols.pickle')\n",
    "ls_models = glob(os.path.join(f'{save_full_path}/', \"catboost_model_fold_*\"))\n",
    "models = [CatBoostClassifier().load_model(fn, format=\"cbm\") for fn in ls_models]\n",
    "\n",
    "cat_features = models[0].feature_names_\n",
    "len(cat_features), len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                        \n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "                    \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 50):\n",
    "                    df = df.drop(col)\n",
    "            \n",
    "            # # eliminate yaer, month feature\n",
    "            # if (col[-1] not in [\"P\", \"A\", \"L\", \"M\"]) and (('month_' in col) or ('year_' in col)):\n",
    "            #     df = df.drop(col)\n",
    "        \n",
    "        # for col in cols_drop:\n",
    "        #     if col in df.columns:\n",
    "        #         df = df.drop(col)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_memory_usage_pl(df):\n",
    "        \"\"\" Reduce memory usage by polars dataframe {df} with name {name} by changing its data types.\n",
    "            Original pandas version of this function: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 \n",
    "        \"\"\"\n",
    "        print(f\"Memory usage of dataframe is {round(df.estimated_size('mb'), 2)} MB\")\n",
    "        \n",
    "        Numeric_Int_types = [pl.Int8, pl.Int16, pl.Int32, pl.Int64]\n",
    "        Numeric_Float_types = [pl.Float32, pl.Float64]    \n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col == 'case_id': \n",
    "                continue\n",
    "            try:\n",
    "                col_type = df[col].dtype\n",
    "                \n",
    "                if col_type == pl.Categorical:\n",
    "                    continue\n",
    "                    \n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                \n",
    "                if col_type in Numeric_Int_types:\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int8))\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int16))\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int32))\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int64))\n",
    "                \n",
    "                elif col_type in Numeric_Float_types:\n",
    "                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Float32))\n",
    "                    else:\n",
    "                        pass\n",
    "                # elif col_type == pl.Utf8:\n",
    "                #     df = df.with_columns(df[col].cast(pl.Categorical))\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "        print(f\"Memory usage of dataframe became {round(df.estimated_size('mb'), 2)} MB\")\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def fill_missing_values(df):\n",
    "        num_cnt = 0\n",
    "        cat_cnt = 0\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype.is_numeric():\n",
    "                df = df.with_columns(pl.col(col).fill_null(-1).alias(col))\n",
    "                num_cnt += 1\n",
    "            else:\n",
    "                df = df.with_columns(pl.col(col).fill_null(\"Missing\").alias(col))\n",
    "                cat_cnt += 1\n",
    "        print(\"num_cnt : \", num_cnt)\n",
    "        print(\"cat_cnt : \", cat_cnt)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] # + [pl.sum(col).alias(f\"sum_{col}\") for col in cols]\n",
    "        # expr_diff = [pl.col(col).diff().alias(f\"diff_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max + [pl.mean(col).alias(f\"mean_{col}\") for col in cols] + [pl.std(col).alias(f\"std_{col}\") for col in cols] # + expr_diff\n",
    "    \n",
    "        # [pl.col(col).drop_nulls().last().alias(f\"last_{col}\") for col in cols]\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)] \n",
    "            \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] # + [pl.sum(col).alias(f\"sum_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max # + [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        \n",
    "        expr_max = [pl.last(col).alias(f\"last_{col}\") for col in cols] + \\\n",
    "            [pl.n_unique(col).alias(f\"n_unique_{col}\") for col in cols] + \\\n",
    "            [pl.first(col).alias(f\"first_{col}\") for col in cols]  # High Value\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] + [pl.sum(col).alias(f\"sum_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max # + [pl.mean(col).alias(f\"mean_{col}\") for col in cols] + [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] # + [pl.n_unique(col).alias(f\"n_unique_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    \n",
    "    if depth in [1]:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    elif depth in [2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    df = df.pipe(Pipeline.reduce_memory_usage_pl)\n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        \n",
    "        if depth in [1]:\n",
    "            df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        elif depth in [2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    \n",
    "    df = df.pipe(Pipeline.reduce_memory_usage_pl)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2, is_train=True):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            decision_month = pl.col(\"date_decision\").dt.month(),\n",
    "            decision_weekday = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    if is_train:\n",
    "        df_base = df_base.pipe(Pipeline.filter_cols)\n",
    "    df_base = df_base.pipe(Pipeline.fill_missing_values)\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-05-05T00:46:21.021879Z\",\"iopub.execute_input\":\"2024-05-05T00:46:21.022207Z\",\"iopub.status.idle\":\"2024-05-05T00:46:21.553731Z\",\"shell.execute_reply.started\":\"2024-05-05T00:46:21.022158Z\",\"shell.execute_reply\":\"2024-05-05T00:46:21.552792Z\"}}\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = feature_eng(**data_store, is_train=False)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "df_test = df_test.select(['case_id', 'WEEK_NUM'] + cat_features)\n",
    "# print(\"train data shape:\\t\", df_train.shape)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def cat_prediction(feats, models):\n",
    "    predictions = np.zeros(len(feats))\n",
    "    for model in models:\n",
    "        p = model.predict_proba(feats)[:, 1]\n",
    "        predictions += p/len(models)\n",
    "    return predictions\n",
    "\n",
    "def predict_proba_in_batches(models, data, batch_size=200000): # about 35min per 10k\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    probabilities = np.zeros((num_samples,))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        print(f\"Processing batch: {batch_idx+1}/{num_batches}\")\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "        \n",
    "        X_batch = data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # batch_probs = joblib.load(f'{automl_path}denselight_model.pkl').predict(X_batch).data.squeeze()\n",
    "        batch_probs = cat_prediction(X_batch, models)\n",
    "        \n",
    "        probabilities[start_idx:end_idx] = batch_probs\n",
    "        \n",
    "        del X_batch\n",
    "        gc.collect()\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "X_test = df_test.drop(columns=[\"WEEK_NUM\"])\n",
    "X_test = X_test.set_index(\"case_id\")\n",
    "print(\"X_test shape: \", df_test.shape)\n",
    "\n",
    "y_pred = pd.Series(predict_proba_in_batches(models, X_test), index=X_test.index)\n",
    "y_pred[:10]\n",
    "\n",
    "subm_df = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\n",
    "subm_df = subm_df.set_index(\"case_id\")\n",
    "subm_df[\"score\"] = y_pred\n",
    "\n",
    "print(subm_df)\n",
    "\n",
    "subm_df.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
